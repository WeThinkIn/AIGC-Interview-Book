<h1 id="目录">目录</h1>

- [1.多模态模型](#1.多模态模型)
	- [1.多模态大模型中，视觉编码器和文本解码器常见的连接方式有哪几种？](#1.多模态大模型中，视觉编码器和文本解码器常见的连接方式有哪几种？)
	- [2.为什么BLIP2中大Q-Former结构不流行了？](#2.为什么BLIP2中大Q-Former结构不流行了？)
- [2.文本大模型](#2.文本大模型)
	- [1.prefix LM 和 causal LM 区别是什么？](#1.prefix和causal区别是什么？)
- [3.通识架构](#3.通识架构)
    - [1.为什么现在的大模型大多是decoder-only的架构？](#1.为什么现在的大模型大多是decoder-only的架构？)
    - [2.旋转位置编码的作用](#2.旋转位置编码的作用)
    - [3.目前主流的开源模型体系有哪些？](#3.目前主流的开源模型体系有哪些？)
    - [4.目前大模型模型结构都有哪些？](#4.目前大模型模型结构都有哪些？)
    - [5.大模型常用的激活函数有哪些？](#5.大模型常用的激活函数有哪些？)
    - [6.GPT3、LLAMA、ChatGLM 的Layer Normalization 的区别是什么？各自的优缺点是什么？](#6.GPT3LLAMAChatGLM的LayerNormalization的区别是什么？各自的优缺点是什么？)
    - [7.Multi-query Attention与Grouped-query Attention区别是什么？](#7.Multi-queryAttention与Grouped-queryAttention区别是什么？)
    - [8.Encoder-decoder架构和decoder-only架构有什么区别？](#8.Encoder-decoder架构和decoder-only架构有什么区别？)
    - [9.非Transformer架构的算法模型如LFM(Liquid Foundation Models)有哪些优势？](#3.非Transformer架构的算法模型如LFM(Liquid-Foundation-Models)有哪些优势？)
- [4.DeepSeek相关问答](#4.DeepSeek相关问答)
    - [1.DeepSeek在算法架构优化方面有哪些具体的技术突破？](#1.DeepSeek在算法架构优化方面有哪些具体的技术突破？)
    - [2.DeepSeek-R1的训练过程中，如何通过RL强化LLM的推理能力？](#2.DeepSeek-R1的训练过程中，如何通过RL强化LLM的推理能力？)
    - [3.DeepSeek-R1模型在哪些方面对DeepSeek-R1-Zero进行了改进？其多阶段训练管道的具体步骤是什么？](#4.DeepSeek-R1模型在哪些方面对DeepSeek-R1-Zero进行了改进？其多阶段训练管道的具体步骤是什么？)
    - [4.请介绍下在DeepSeek-R1中所用到的GRPO算法？](#4.请介绍下在DeepSeek-R1中所用到的GRPO算法？)
    - [5.请介绍下在DeepSeek-R1中所用到的MLA算法？](#5.请介绍下在DeepSeek-R1中所用到的MLA算法？)
    - [6.请介绍下在DeepSeek-R1中所用到的冷启动数据？](#6.请介绍下在DeepSeek-R1中所用到的冷启动数据？)
    - [7.请介绍下在DeepSeek-R1中所用到的GPU优化技术？](#7.请介绍下在DeepSeek-R1中所用到的GPU优化技术？)
    - [8.请介绍下DeepSeek-R1训练过程中的拒绝采样和监督微调](#8.请介绍下DeepSeek-R1训练过程中的拒绝采样和监督微调)
    - [9.DeepSeek-V3、DeepSeek-R1—Zero、DeepSeek-R1之间的联系？](#9.DeepSeek-V3、DeepSeek-R1—Zero、DeepSeek-R1之间的联系？)

<h2 id='1.多模态模型'>1.多模态模型</h2>


<h3 id='1.多模态大模型中，视觉编码器和文本解码器常见的连接方式有哪几种？'>1.多模态大模型中，视觉编码器和文本解码器常见的连接方式有哪几种？</h3>

常见连接方式有Q-Former，Attention，Linear Layer/ MLP结构。此外还有Fuyu这类较特殊的结构，它没有Image Encoder，而是直接把image patches通过Linear Layer映射后送入LLM。

各结构的代表性方法列举如下：

**Q-Former**

>以BLIP-2为代表的Q-Former结构在其中增加了多个目标函数，希望视觉信息和文本信息在Q-Former中进一步对齐。

![BLIP2整体结构](imgs/基础知识/BLIP2-1.png)

![BLIP2 Q-Former结构](imgs/基础知识/BLIP2-2.png)


**Attention**

>以Flamingo结构为代表的Attention结构没有简单的把视觉tokens和文本tokens拼接到一起，而是在cross-attention层加入，增强了视觉信息和文本信息间的交互。

![Flamingo整体结构](imgs/基础知识/Flamingo-1.png)

![Flamingo attention](imgs/基础知识/Flamingo-2.png)


**Linear Layer / MLP**

>最近的研究工作大大简化的连接方式，以LLaVA为代表的方法仅使用了一个Linear Layer作为连接器，然后把视觉tokens和文本tokens经过拼接后送入LLM。

>在LLaVA 1.5中，Linear Layer升级为了2层MLP。目前MLP结构广受欢迎。

![LLaVA1 Linear Layer](imgs/基础知识/LLaVA1.png)


**Fuyu**

>Fuyu架构同样使用了Linear Layer，但更为特殊的是，Fuyu索性将image encoder去掉了，直接将image patches经Linear Layer映射后与文本tokens拼接，并送入LLM中。

![Fuyu架构](imgs/基础知识/fuyu.png)

<h3 id='2.为什么BLIP2中大Q-Former结构不流行了？'>2.为什么BLIP2中大Q-Former结构不流行了？</h3>

1. LLaVA系列的流行使很多后续工作follow了MLP结构；

2. 在Q-Former结构没有获得比MLP结构更优性能的前提下，使用简单易收敛的MLP结构何乐而不为；

3. Q-Former的有损压缩结构会损失视觉信息，导致模型容易产生幻觉。


<h2 id='2.文本大模型'>2.文本大模型</h2>

<h3 id='1.prefix和causal区别是什么？'>1.prefix LM 和 causal LM 区别是什么？</h3>

前缀语言模型（Prefix LM）利用给定前缀的全局上下文进行文本生成和填空，适用于需要结合全局信息的任务，如自然语言理解和填空任务；

而因果语言模型（Causal LM）按序列顺序逐字生成文本，依赖前面词预测下一个词，主要用于自回归生成任务，如文本生成和对话生成。



<h2 id='3.通识架构'>3.通识架构</h2>


<h3 id='1.为什么现在的大模型大多是decoder-only的架构？'>1.为什么现在的大模型大多是decoder-only的架构？</h3>

LLM之所以主要都用Decoder-only架构，除了训练效率和工程实现上的优势外，在理论上是因为Encoder的双向注意力会存在低秩问题，这可能会削弱模型表达能力，就生成任务而言，引入双向注意力并无实质好处。而Encoder-Decoder架构之所以能够在某些场景下表现更好，大概只是因为它多了一倍参数。所以，在同等参数量、同等推理成本下，Decoder-only架构就是最优选择了。

<h3 id='2.旋转位置编码的作用'>2.旋转位置编码的作用</h3>

### 旋转位置编码的本质和计算流程

旋转位置编码RoPE是一种固定式的绝对位置编码策略，但是它的绝对位置编码配合Transformer的Attention内积注意力机制能达到相对位置编码的效果。RoPE的本质是对两个token形成的Query和Key向量做一个变换，使得变换后的Query和Key带有位置信息，进一步使得Attention的内积操作不需要做任何更改就能自动感知到相对位置信息。换句话说，RoPR的出发点和策略用的相对位置编码思想，但是实现方式的确用的是绝对位置编码。

固定式表明RoPE没有额外需要模型自适应学习的参数，因此RoPE是一种高效的编码方式。绝对位置编码表明RoPE给文本的每个位置单词都分配了一个位置表征，和三角sin-cos位置编码一样，RoPE通过token在句子中的位置，token embedding中每个元素的位置，这两个要素一起确定位置编码的表达

### 旋转位置编码如何表达相对位置信息

sin-cos位置编码因为三角函数的性质，使得它可以表达相对位置信息，具体而言是：给定距离，任意位置的位置编码都可以表达为一个已知位置的位置编码的关于距离的线性组合，而RoPE的位置编码也是同样的思路，采用绝对位置编码实现相对距离的表达，区别如下:

- 实现相对位置能力的途径不同：sin-cos位置编码由于三角函数的性质，导致它本身就具备表达相对距离的能力，而RoPE位置编码本身不能表达相对距离，需要结合Attention的内积才能激发相对距离的表达能力

- 和原输入的融合计算方式不同：sin-cos位置编码直接和原始输入相加，RoPE位置编码采用类似哈达马积相乘的形式。


<h3 id='3.目前主流的开源模型体系有哪些？'>3.目前主流的开源模型体系有哪些？</h3>

目前主流的开源模型体系主要包括以下几个：

1. Transformer及其变体：

   包括Google提出的Transformer模型以及基于Transformer架构的各种变体，如BERT (Bidirectional Encoder Representations from Transformers)、GPT (Generative Pre-trained Transformer) 等。这些模型在自然语言处理任务中取得了显著的成就。
  
2. BERT（Bidirectional Encoder Representations from Transformers）：

   BERT 是一种预训练语言模型，采用Transformer编码器架构，并通过大规模无监督训练来学习语言表示。它能够通过微调在多种NLP任务中达到很高的性能。
   
3. GPT（Generative Pre-trained Transformer）：

   GPT 系列模型是基于Transformer解码器架构的预训练语言模型，主要用于生成式任务和文本生成。
   
4. PyTorch Lightning：

   pyTorch Lightning 是一个基于PyTorch的轻量级深度学习框架，旨在简化模型训练过程，并提供可扩展性和复现性。
   
5. TensorFlow Model Garden：

   TensorFlow Model Garden 提供了 TensorFlow 官方支持的一系列预训练模型和模型架构，涵盖了多种任务和应用领域。
   
6. Hugging Face Transformers：

   Hugging Face Transformers 是一个流行的开源库，提供了大量预训练模型和工具，特别适用于自然语言处理任务。它使得研究人员和开发者能够轻松使用、微调和部署各种现成的语言模型。
   
   这些开源模型体系在机器学习和自然语言处理领域都有广泛的应用和影响力，为研究人员和开发者提供了强大的工具和资源。
   
   
<h3 id='4.目前大模型模型结构都有哪些？'>4.目前大模型模型结构都有哪些？</h3>

目前大模型的模型结构主要包括以下几种：

1. Transformer模型：

   原始Transformer：基础模型，采用自注意力机制和前馈神经网络。
   
   GPT系列：基于自回归生成的Transformer变体，适用于文本生成任务。
   
   BERT系列：基于双向编码的Transformer变体，适用于自然语言理解任务。
   
   T5：结合生成和理解的Transformer，使用统一的文本到文本框架。
   
   LLAMA：类似GPT，但采用前标准化结构，提高泛化能力和鲁棒性
   
2. 混合结构模型：

   Transformer-XL：在Transformer中引入相对位置编码和片段级记忆机制，处理长序列任务。
   
   XLNet：融合自回归和自编码思想，通过双向学习提升模型性能。
   
3. 稠密模型：

   DeBERTa：结合相对位置编码和解耦的注意力机制，提高模型性能和泛化能力。
   
4. 稀疏模型：

   Switch Transformer：通过稀疏激活和专家混合机制，实现大规模训练和推理的高效性。
   
   GShard：在大规模并行计算框架下优化Transformer的性能
   
5. 对比学习模型：

   SimCLR：利用对比学习方法进行预训练，增强模型的表示能力。
   
   CLIP：将图像和文本进行对比学习，获取多模态表示。

这些模型结构在不同的任务和应用场景中展现了各自的优势和特点，不断推动自然语言处理和生成模型的发展。


<h3 id='5.大模型常用的激活函数有哪些？'>5.大模型常用的激活函数有哪些？</h3>

大模型常用的激活函数包括ReLU、Leaky ReLU、ELU、Swish和GELU。ReLU计算简单且有效避免梯度消失问题，加快训练速度，但可能导致神经元死亡；Leaky ReLU通过引入小斜率缓解ReLU的缺点；GeLU一种改进的ReLU函数，可以提供更好的性能和泛化能力；Swish一种自门控激活函数，可以提供非线性变换，并具有平滑和非单调的特性，在平滑性和性能上表现优异，但计算开销较大。


<h3 id='6.GPT3LLAMAChatGLM的LayerNormalization的区别是什么？各自的优缺点是什么？'>6.GPT3、LLAMA、ChatGLM 的Layer Normalization 的区别是什么？各自的优缺点是什么？</h3>

GPT-3：采用的是后标准化结构，即在执行自注意力或前馈神经网络计算之后进行Layer Normalization。这种方法有助于稳定训练过程并提升模型性能。

LLAMA：使用前标准化结构，即在自注意力或前馈神经网络计算之前进行Layer Normalization。这种结构有助于提升模型的泛化能力和鲁棒性。

ChatGLM：与GPT-3相似，采用后标准化结构，即在自注意力或前馈神经网络计算之后进行Layer Normalization。这种方法能够增强模型的性能和稳定性。


<h3 id='7.Multi-queryAttention与Grouped-queryAttention区别是什么？'>7.Multi-query Attention与Grouped-query Attention区别是什么？</h3>

Multi-query Attention和Grouped-query Attention是两种改进和扩展传统自注意力机制的变体。
Multi-query Attention：在这种机制中，每个查询与多个键值对进行交互，从而能够捕捉更多的上下文信息。这有助于提高模型在处理长序列或复杂关系时的表达能力和性能。

Grouped-query Attention：这种机制将查询分成多个组，每个组内的查询与相应的键值对进行交互。这样可以减少计算复杂度，提高效率，同时仍能保持良好的性能。


<h3 id='8.Encoder-decoder架构和decoder-only架构有什么区别？'>8.Encoder-decoder架构和decoder-only架构有什么区别？</h3>

- **Encoder-only架构**：只有编码器的模型，如BERT模型，能够很好的注意到输入文本的语义和上下文关系，但不擅长生成内容，适用于文本分类，情感分析等领域。

- **Decoder-only架构**： 仅含有解码器的模型，如GPT模型，不太擅长理解主题和学习目标，更关注于从已有的信息扩展出新的内容，适用于创造性的写作。

- **Encoder-Decoder架构**： 同时包含编码器和解码器部分的模型，如T5模型。该架构利用编码器对输入序列进行编码，提取其特征和语义信息，并将编码结果传递给解码器；然后，解码器根据编码结果生成相应的输出序列，适用于文本翻译等领域。


<h3 id='9.非Transformer架构的算法模型如LFM(Liquid-Foundation-Models)有哪些优势？'>9.非Transformer架构的算法模型如LFM(Liquid Foundation Models)有哪些优势？</h3>

- 其小巧便携的特性使得它能够直接部署在手机上进行文档和书籍等分析；
- 超越了同等规模的Transformer模型如Llama 3.2，性能更优；
- 基于动态系统理论、信号处理和数值线性代数，计算单元设计更加高效。


<h2 id='4.DeepSeek相关问答'>4.DeepSeek相关问答</h2>

<h3 id='1.DeepSeek在算法架构优化方面有哪些具体的技术突破？'>1.DeepSeek在算法架构优化方面有哪些具体的技术突破？</h3>

**1）重新设计了训练流程**，通过少量SFT数据和多轮强化学习的方法，提高了模型准确性，同时显著降低了内存占用和计算开销；

**2）实现了算力与性能的近似线性关系**，每增加一张GPU，模型推理能力可稳定提升，无需依赖复杂的外部监督机制。


<h3 id='2.DeepSeek-R1的训练过程中，如何通过RL强化LLM的推理能力？'>2.DeepSeek-R1的训练过程中，如何通过RL强化LLM的推理能力？</h3>

DeepSeek-R1的训练过程中，**R1-zero完全基于RL进行训练**，未使用任何监督训练或人类反馈。
通过自我学习，R1-zero能够提高性能。R1则是在R1-zero的基础上，通过**少量冷启动数据进行微调**，提高了输出质量和可读性。


<h3 id='3.DeepSeek-R1模型在哪些方面对DeepSeek-R1-Zero进行了改进？其多阶段训练管道的具体步骤是什么？'>3.DeepSeek-R1模型在哪些方面对DeepSeek-R1-Zero进行了改进？其多阶段训练管道的具体步骤是什么？</h3>

**DeepSeek-R1模型在以下几个方面对DeepSeek-R1-Zero进行了改进**：

- **可读性**：通过引入冷启动数据和多阶段训练管道，DeepSeek-R1解决了DeepSeek-R1-Zero内容可读性差和语言混合的问题。
- **推理性能**：通过额外的监督和微调阶段，DeepSeek-R1在推理任务上达到了与OpenAI-o1-1217相当的水平。

**多阶段训练管道的具体步骤如下**：

- **冷启动数据收集**：收集数千条冷启动数据，用于微调DeepSeek-V3-Base模型。这些数据包括详细的答案和反思过程。
- **推理导向的强化学习**：在冷启动数据微调后的模型上进行推理导向的强化学习，使用Group Relative Policy Optimization（GRPO）算法进行优化。
- **拒绝采样和SFT**：通过拒绝采样和SFT生成新的SFT数据，并结合DeepSeek-V3的监督数据进行再训练。
- **额外的RL过程**：最终通过额外的RL过程，结合所有场景的提示，得到DeepSeek-R1模型。


<h3 id='4.请介绍下在DeepSeek-R1中所用到的GRPO算法？'>4.请介绍下在DeepSeek-R1中所用到的GRPO算法？</h3>

**GRPO**是一种策略优化算法，旨在减少训练成本并提高模型的性能。GRPO的主要特点是不使用与策略模型相同大小的批评模型（critic model），
而是通过从一组分数中估计基线来优化策略模型。这种方法在处理大规模策略优化时特别有用，因为它可以显著降低计算资源的需求。

**GRPO 算法的核心思想:**

**1. 策略优化：**

GRPO 通过最大化一个目标函数来优化策略模型。这个目标函数结合了策略的改进和策略的稳定性。

**2. 基线估计：**

传统的策略优化方法通常需要一个批评模型来估计策略的价值函数，以便计算优势函数。然而，GRPO 通过从一组策略输出中估计基线来避免使用单独的批评模型。这种方法利用了组内样本之间的相对信息来估计基线。

**3. 优势函数：**

优势函数用于衡量策略改进的程度。在 GRPO 中，优势函数是基于一组奖励计算的，这些奖励对应于每个组内的输出。
![](imgs/GRPO公式.png)


<h3 id='5.请介绍下在DeepSeek-R1中所用到的MLA算法？'>5.请介绍下在DeepSeek-R1中所用到的MLA算法？</h3>

Multi-Head Latent Attention (MLA) 是 DeepSeek-R1 模型中用于高效推理的核心注意力机制。MLA 通过低秩联合压缩技术，减少了推理时的键值（KV）缓存，从而在保持性能的同时显著降低了内存占用。

### 基本概念
在标准的 Transformer 模型中，多头注意力（Multi-Head Attention, MHA）机制通过并行计算多个注意力头来捕捉输入序列中的不同特征。每个注意力头都有自己的查询（Query, Q）、键（Key, K）和值（Value, V）矩阵，计算过程如下：

- 查询矩阵 Q：用于计算输入序列中每个位置的注意力权重。
- 键矩阵 K：用于与查询矩阵 Q 计算注意力分数。
- 值矩阵 V：用于根据注意力分数加权求和，得到最终的输出。
MLA 的核心思想是通过低秩联合压缩技术，减少 K 和 V 矩阵的存储和计算开销。

### 具体实现
MLA 的具体实现过程如下：

1. **压缩键和值**： 设输入序列的第t个token的嵌入向量为$h_t \in \mathbb{R}^d$，其中d是嵌入维度。

通过一个下投影矩阵$W^{DKV} \in \mathbb{R}^{d_c \times d}$，将$h_t$压缩为一个低维的潜在向量$c_t^{KV} \in \mathbb{R}^{d_c}$，其中$d_c \ll d_h n_h$，$d_h$是每个注意力头的维度，$n_h$是注意力头的数量。

$c_t^{KV} = W^{DKV} h_t$ 

2. **重建键和值**： 通过上投影矩阵$W^{UK} \in \mathbb{R}^{d_{n_h} \times d_c}$和$W^{UV} \in \mathbb{R}^{d_{n_h} \times d_c}$，将压缩后的潜在向量$\mathbf{c}^{KV}$重建为键和值矩阵：
$${\mathbf{k}_t^C} = W^{UK} \mathbf{c}_t^{KV}$$
$${\mathbf{v}_t^C} = W^{UV} \mathbf{c}_t^{KV}$$

3. **应用旋转位置编码（RoPE）**：为了引入位置信息，MLA对键矩阵应用旋转位置编码（RoPE）：
$k_t^R = RoPE(W^{KR}h_t)$
其中，$W^{KR} \in \mathbb{R}^{d_h^R \times d}$是用于生成解耦键的矩阵，$d_h^R$是解耦键的维度。 

4. **最终键和值**： 最终的键和值矩阵由压缩后的键和值以及旋转位置编码后的键组合而成:
$k_t = [k_t^C; k_t^R]$
$v_t = v_t^C$ 

### 查询矩阵的低秩压缩

1. **压缩查询**：
MLA 还对查询矩阵 Q 进行低秩压缩，以减少训练时的激活内存：
 通过下投影矩阵$W^{DQ} \in \mathbb{R}^{d_{h} \times d}$，将$\mathbf{h}_{t}$压缩为一个低维的潜在向量$\mathbf{c}_{t}^{Q} \in \mathbb{R}^{d_{c}}$，其中$d_{c} \ll d_{h} n_{h}$。
$\mathbf{c}_{t}^{Q} = W^{DQ} \mathbf{h}_{t}$

2. **重建查询**：
通过上投影矩阵$W^{UQ} \in \mathbb{R}^{d_{sh}n_{h} \times d_c}$，将压缩后的潜在向量$\mathbf{c}_t^Q$重建为查询矩阵：
$
\mathbf{q}_t^C = W^{UQ} \mathbf{c}_t^Q
$ 
3. **应用旋转位置编码（RoPE）**：对查询矩阵应用旋转位置编码
$q_t^R = RoPE(W^{QR}c_t^Q)$
其中, $W^{QR} \in \mathbb{R}^{d_R n_h \times d_z}$ 是用于生成解耦查询的矩阵。
4. **最终查询**： 最终的查询矩阵由压缩后的查询和旋转位置编码后的查询组合而成:
$q_t = [q_t^C; q_t^R]$

### 注意力计算
1. **计算注意力分数**： 使用重建后的查询矩阵和键矩阵计算注意力分数,对于每个注意力头i，计算查询$q_{t,i}$和键$k_{j,i}$的点积，并除以${\sqrt{d_h + d^R_h}}$进行缩放:

$score_{j,t,i} = \frac{\mathbf{q}_{t,i}^T \mathbf{k}_{j,i}}{\sqrt{d_h + d^R_h}}$

2. **计算注意力权重**：对注意力分数进行softmax归一化，得到注意力权重：
$α_{t,j,\hat{i}} = softmax_j(score_{t,j,\hat{i}})$ 

3. **加权求和**：使用注意力权重绝对值${v_{j,i}^{C}}$进行加权求和，得到每个注意力头的输出：
$o_{t,i}=\sum_{j=1}^{t} \alpha_{t,j,i} v_{j,i}^{C}$
4. **合并多头输出**：将所有注意力头的输出拼接起来，并通过输出投影矩阵$W^O \in \mathbb{R}^{d \times d_{v h}}$进行线性变换，得到最终的注意力输出：
$\mathbf{u}_t = W^O [\mathbf{o}_{t,1};\mathbf{o}_{t,2}; \cdots ;\mathbf{o}_{t,n_h}]$

### 总结
Multi-Head Latent Attention (MLA) 通过低秩联合压缩技术，显著减少了推理时的键值缓存和训练时的激活内存，同时保持了与标准多头注意力机制相当的性能。MLA 的核心在于对键、值和查询矩阵进行低秩压缩，并通过旋转位置编码引入位置信息，从而在高效推理的同时捕捉输入序列中的复杂特征。


<h3 id='6.请介绍下在DeepSeek-R1中所用到的冷启动数据？'>6.请介绍下在DeepSeek-R1中所用到的冷启动数据？</h3>

### 冷启动数据的作用
- 让 AI 训练更稳定：避免 AI 训练初期陷入“胡乱生成答案”的混乱状态。
- 提升推理质量：让 AI 在强化学习前就具备一定的推理能力，而不是完全从零开始。
- 改善语言表达：减少 AI 生成的语言混杂和重复内容，让推理过程更清晰、可读性更高。

### 冷启动数据的构建
- 使用少样本 (few-shot) 提示，以长思维链作为示例，引导模型生成详细推理过程。
- 直接提示模型生成包含反思 (reflection) 和验证 (verification) 的完整回答。
- 提取 DeepSeek-R1-Zero 的推理结果，并以可读格式进行重构。
- 人工后处理 (post-processing)，对模型输出进行优化，使其更加清晰易读。

相比较DeepSeek-R1-Zero，冷启动数据有以下主要优势：
- 可读性 (Readability)：DeepSeek-R1-Zero 的回答往往缺乏可读性，例如混合多种语言，或未使用 Markdown 格式高亮答案。而在 DeepSeek-R1 的冷启动数据中，特别设计了可读性模式 (readable pattern)，在每个回答的末尾添加摘要，并筛选出不适合阅读的回答。其输出格式为：
|special_token|<reasoning_process>|special_token|<summary>
其中，<reasoning_process> 代表用于 query 的思维链，<summary> 用于总结推理结果。
- 潜力 (Potential)：通过在冷启动数据设计中引入 人类先验知识 (human priors)，DeepSeek-R1 相比 DeepSeek-R1-Zero 展现出了更好的推理表现。


<h3 id='7.请介绍下在DeepSeek-R1中所用到的GPU优化技术？'>7.请介绍下在DeepSeek-R1中所用到的GPU优化技术？</h3>

在DeepSeek-V3的论文中提到，研究团队在英伟达的H800 GPU上做了深入优化，实现了高达10 倍计算效率提升（相较于 Meta 等团队）。
核心优化手段包括：

1) 深度优化 PTX 代码
DeepSeek 直接操作 PTX（Parallel Thread Execution） 代码，而不是使用 CUDA 高级 API。PTX 是英伟达 GPU 计算架构中的中间表示层，接近汇编语言，允许对寄存器分配、线程管理、流式处理等进行更细粒度的优化。

为什么使用 PTX 而非 CUDA？
- CUDA 适合通用编程，但封装了一层 API，限制了一些极致优化操作。
- PTX 允许定制化调度、寄存器分配，使计算资源利用率达到极限。

2) 服务器通信优化
DeepSeek 团队重新定义了 132 个流式多处理器（SMs）中的 20 个，将其转换为服务器间通信处理单元，从而绕过了 GPU 内部的通信瓶颈。这种优化策略进一步提升了计算效率。

但 PTX 代码的缺陷是？

- 代码极难维护，通常只有少数顶级团队可以驾驭。
- 难以移植到不同 GPU，特定于 H800 的优化可能在 A100 或其他 GPU 上失效。

<h3 id='8.请介绍下DeepSeek-R1训练过程中的拒绝采样和监督微调'>8.请介绍下DeepSeek-R1训练过程中的拒绝采样和监督微调</h3>

当面向推理的 RL 训练收敛后，利用当前模型的 Checkpoint 生成监督微调 (SFT) 数据，用于下一轮训练。与初始冷启动数据不同，这一阶段不仅关注推理任务，还包含写作 (writing)、角色扮演 (role-playing) 和其他通用任务的数据，以增强模型的多功能性。

1) 推理数据 (Reasoning Data) 处理流程:
- 采样 (sampling)：我们从 RL 训练的 Checkpoint 采样推理任务数据，并进行拒绝采样 (Rejection Sampling)，确保仅保留高质量的推理结果。
- 扩展数据集：在上一阶段，仅包含可通过基于规则的奖励评估的数据，而本阶段扩展数据来源，部分数据使用 生成式奖励模型 (Generative Reward Model)，通过将真实答案和模型预测输入 DeepSeek-V3 进行评估。
- 数据清理：由于模型输出有时会混杂多种语言、长段文本或代码块，筛选出语言混杂的思维链、冗长段落和不清晰代码，确保最终数据更具可读性。
- 最终数据集：为每个任务采样多个响应，并仅保留正确的回答，最终收集了约 60 万条与推理相关的训练数据。

2) 非推理数据 (Non-Reasoning data)：

- 对于非推理数据，例如写作、事实问答、自我认知和翻译，采用 DeepSeek-V3 流水线，并复用了部分 DeepSeek-V3 的 SFT 数据集。
- 对于某些非推理任务，调用 DeepSeek-V3 生成潜在的思维链（CoT），然后再通过提示进行回答。然而，对于诸如 “你好” 之类的简单 query，不会提供 CoT 作为响应。
- 最终，共收集了大约 20 万条与推理无关的训练样本。使用上述约 80 万条精心整理的数据集，对 DeepSeek-V3-Base 进行两轮微调。


<h3 id='9.DeepSeek-V3、DeepSeek-R1—Zero、DeepSeek-R1之间的联系？'>9.DeepSeek-V3、DeepSeek-R1—Zero、DeepSeek-R1之间的联系？</h3>

### DeepSeek-V3
DeepSeek-V3是一个基于Transformer架构的混合专家模型（MoE），总参数量达671B，每个Token激活37B参数，专注于高效训练和通用自然语言处理任务（如文本生成、多轮对话等）

### DeepSeek-R1-Zero
基于DeepSeek-V3基座模型，纯强化学习（RL）训练的推理专用模型，未经过监督微调（SFT），展示了自我验证、反思和长思维链（CoT）生成能力，但是输出格式混乱（如混合语言）、可读性差，且未覆盖通用任务   

### DeepSeek-R1
在R1-Zero基础上引入监督微调（SFT）和多阶段强化学习，平衡推理能力与用户友好性，并扩展至更广泛的任务场景。

DeepSeek-R1是如何产生的？

1) 基础模型构建：DeepSeek-V3 基座，混合专家模型（MOE），总参数量达 671B，每个 Token 激活 37B 参数，提供强大的通用任务处理能力，支持多 Token 预测和负载均衡策略，优化训练效率，但在复杂推理任务上表现有限。

2) 纯强化学习阶段：R1-Zero 的冷启动，通过纯强化学习（无监督微调）探索模型的推理能力涌现，采用 GRPO（组相关策略优化），取消对价值函数模型的依赖，通过组内平均奖励优化策略，降低计算复杂度；在奖励机制方面主要是是准确性奖励：验证数学答案的正确性和格式奖励：强制模型将推理过程置于 \<think> 标签内，提升可读性；并要求模型先输出推理步骤，再给出最终答案，强化逻辑表达

3) 监督微调（SFT）与冷启动优化，解决纯 RL 训练中输出格式混乱、语言混杂的问题，提升模型稳定性。使用数千条人工标注的高质量数据，包括长思维链（CoT）示例和 R1-Zero 输出的格式化结果，对 V3 基座进行微调，注入易读性模板（如语言一致性奖励），确保推理步骤清晰。

4) 多阶段强化学习训练
- 第一阶段：推理密集型任务优化
   - GRPO 应用：结合规则奖励（数学、代码任务）和语言一致性奖励（目标语言占比），提升推理能力
   - 拒绝采样（RS）：生成合成数据集，覆盖写作、角色扮演等通用任务，扩展模型能力
- 第二阶段：全场景强化学习
   - 混合奖励机制：
      - 规则奖励：用于数学、代码等确定性任务。
      - 模型奖励：用于开放式问答、创意写作等主观任务812。
   - 多任务优化：通过不同提示分布和奖励信号，平衡模型在推理与通用任务上的表现

